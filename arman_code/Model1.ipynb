{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31cfe405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import math\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "import xarray\n",
    "import rioxarray\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "addeba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function that handles the daily scraping of our source URL\n",
    "\n",
    "#list of states by abbrv. - the lowercase state abbrv. are used in the url that we scrape from\n",
    "states = ['AL','AK','AZ','AR', 'CA','CO','CT','DE','FL','GA','HI','ID','IL','IN',\n",
    "'IA','KS','KY','LA','ME','MD','MA','MI','MN','MS','MO','MT','NE','NV','NH','NJ','NM','NY',\n",
    "'NC','ND','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VT','VA','WA','WV','WI','WY']\n",
    "states = list(map(str.lower,states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc204605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the driver and set our starting url\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22f924e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which scrapes current state of all of our USGS stations available with flood state information\n",
    "\n",
    "def TableScrape():\n",
    "    url = 'https://waterwatch.usgs.gov/index.php?id=flood&sid=w__table&r=ri'\n",
    "    df = pd.DataFrame()\n",
    "    for x in range(len(states)):\n",
    "        url = url[:-2]+states[x]\n",
    "        driver.get(url)\n",
    "        hold_df = pd.read_html(url)[4:]\n",
    "        hold_df = hold_df[0]\n",
    "        df = df.append(hold_df, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cae5eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\2734864620.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(hold_df, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "#Instantiate a dataframe that contains all of our USGS flooding information\n",
    "df = TableScrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd06ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all of our functions to scrape USGS gauge information at a more granular level\n",
    "# for streamflow, surface water level, ground water level, spring water level, and precipitation\n",
    "\n",
    "# dicts containing all the information that pertains to our percipitation data from USGS map\n",
    "# meaning that this is the network data that is called from https://dashboard.waterdata.usgs.gov/app/nwd/en/?region=lower48&aoi=default\n",
    "# the value in question for these gauges is percipitation - total inches\n",
    "\n",
    "def getPrecipitation():\n",
    "    headers_precip = {\n",
    "        'Accept': '*/*',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Connection': 'keep-alive',\n",
    "        # 'Cookie': '_hjSessionUser_606685=eyJpZCI6Ijg0MTMwZDAyLWJjZTYtNTYwMC1hNzhlLTUxNDA2ZGI0YWY1NyIsImNyZWF0ZWQiOjE2ODM3MzMyNTg5MzUsImV4aXN0aW5nIjp0cnVlfQ==; _ga=GA1.3.2126571673.1683733259; _ga=GA1.4.2126571673.1683733259; __utmc=191289900; _gid=GA1.2.1982434073.1684175396; _ga_Q3BG15MFSR=GS1.1.1684253524.4.0.1684253528.0.0.0; _ga_BTG7QB0V1W=GS1.1.1684338231.2.0.1684338231.0.0.0; _gid=GA1.4.1982434073.1684175396; __utma=191289900.2126571673.1683733259.1684427918.1684429964.7; __utmz=191289900.1684429964.7.5.utmcsr=dashboard.waterdata.usgs.gov|utmccn=(referral)|utmcmd=referral|utmcct=/; _ga_232G9MDQL1=GS1.1.1684507551.13.0.1684507553.0.0.0; _ga=GA1.2.2126571673.1683733259; _gat_UA-179298358-1=1',\n",
    "        'Referer': 'https://dashboard.waterdata.usgs.gov/app/nwd/en/?region=lower48&aoi=default',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36',\n",
    "        'sec-ch-ua': '\"Chromium\";v=\"112\", \"Google Chrome\";v=\"112\", \"Not:A-Brand\";v=\"99\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "    }\n",
    "\n",
    "    response_precipitation = requests.get(\n",
    "        'https://dashboard.waterdata.usgs.gov/service/cwis/1.0/odata/CurrentConditions?$top=15000&$filter=(AccessLevelCode%20eq%20%27P%27)%20and%20(1%20eq%201%20and%20true)%20and%20(ParameterCode%20in%20(%2772192%27%2C%2772194%27%2C%2799772%27%2C%2700045%27%2C%2700193%27))&$select=AgencyCode,SiteNumber,SiteName,SiteTypeCode,Latitude,Longitude,CurrentConditionID,ParameterCode,TimeLocal,TimeZoneCode,Value,ValueFlagCode,RateOfChangeUnitPerHour,StatisticStatusCode,FloodStageStatusCode&$orderby=SiteNumber,AgencyCode,ParameterCode,TimeLocal%20desc&caller=National%20Water%20Dashboard%20default',\n",
    "        headers=headers_precip,\n",
    "    )\n",
    "    \n",
    "    return response_precipitation.json()['value']\n",
    "\n",
    "# dicts containing all the information that pertains to our spring water level data from USGS map\n",
    "# meaning that this is the network data that is called from https://dashboard.waterdata.usgs.gov/app/nwd/en/?region=lower48&aoi=default\n",
    "# the value in question for these is gauge height \n",
    "def getSpringWaterLevel():\n",
    "    headers_spring = {\n",
    "        'Accept': '*/*',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Referer': 'https://dashboard.waterdata.usgs.gov/app/nwd/en/?region=lower48&aoi=default',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36',\n",
    "        'sec-ch-ua': '\"Chromium\";v=\"112\", \"Google Chrome\";v=\"112\", \"Not:A-Brand\";v=\"99\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "    }\n",
    "\n",
    "    response_spring = requests.get(\n",
    "        'https://dashboard.waterdata.usgs.gov/service/cwis/1.0/odata/CurrentConditions?$top=15000&$filter=(AccessLevelCode%20eq%20%27P%27)%20and%20(1%20eq%201%20and%20true)%20and%20(SiteTypeCode%20in%20(%27SP%27))%20and%20(ParameterCode%20in%20(%2730207%27%2C%2730210%27%2C%2730211%27%2C%2730212%27%2C%2730213%27%2C%2762600%27%2C%2762601%27%2C%2762610%27%2C%2762611%27%2C%2762612%27%2C%2762613%27%2C%2762614%27%2C%2762615%27%2C%2762616%27%2C%2762617%27%2C%2762618%27%2C%2762619%27%2C%2762620%27%2C%2762621%27%2C%2762622%27%2C%2762623%27%2C%2762624%27%2C%2763158%27%2C%2763159%27%2C%2763160%27%2C%2763161%27%2C%2772019%27%2C%2772020%27%2C%2772150%27%2C%2772170%27%2C%2772171%27%2C%2772178%27%2C%2772199%27%2C%2772214%27%2C%2772215%27%2C%2772226%27%2C%2772227%27%2C%2772228%27%2C%2772229%27%2C%2772230%27%2C%2772231%27%2C%2772232%27%2C%2772251%27%2C%2772264%27%2C%2772265%27%2C%2772275%27%2C%2772276%27%2C%2772279%27%2C%2772292%27%2C%2772293%27%2C%2772335%27%2C%2772336%27%2C%2799019%27%2C%2799020%27%2C%2799065%27%2C%2799227%27%2C%2700062%27%2C%2700065%27%2C%2700072%27))&$select=AgencyCode,SiteNumber,SiteName,SiteTypeCode,Latitude,Longitude,CurrentConditionID,ParameterCode,TimeLocal,TimeZoneCode,Value,ValueFlagCode,RateOfChangeUnitPerHour,StatisticStatusCode,FloodStageStatusCode&$orderby=SiteNumber,AgencyCode,ParameterCode,TimeLocal%20desc&caller=National%20Water%20Dashboard%20default',\n",
    "        headers=headers_spring,\n",
    "    )\n",
    "    \n",
    "    return response_spring.json()['value']\n",
    "\n",
    "# dicts containing all the information that pertains to our ground water level data from USGS map\n",
    "# meaning that this is the network data that is called from https://dashboard.waterdata.usgs.gov/app/nwd/en/?region=lower48&aoi=default\n",
    "# the value in question for these is depth to water - feet below land surface\n",
    "def getGroundWaterLevel():\n",
    "    headers_gwl = {\n",
    "        'Accept': '*/*',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Referer': 'https://dashboard.waterdata.usgs.gov/app/nwd/en/?region=lower48&aoi=default',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36',\n",
    "        'sec-ch-ua': '\"Chromium\";v=\"112\", \"Google Chrome\";v=\"112\", \"Not:A-Brand\";v=\"99\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "    }\n",
    "\n",
    "    response_gwl = requests.get(\n",
    "        'https://dashboard.waterdata.usgs.gov/service/cwis/1.0/odata/CurrentConditions?$top=15000&$filter=(AccessLevelCode%20eq%20%27P%27)%20and%20(1%20eq%201%20and%20true)%20and%20(SiteTypeCode%20in%20(%27GW%27%2C%27GW-CR%27%2C%27GW-EX%27%2C%27GW-HZ%27%2C%27GW-IW%27%2C%27GW-MW%27%2C%27GW-TH%27%2C%27SB%27%2C%27SB-CV%27%2C%27SB-GWD%27%2C%27SB-TSM%27%2C%27SB-UZ%27))%20and%20(ParameterCode%20in%20(%2730207%27%2C%2730210%27%2C%2730211%27%2C%2730212%27%2C%2730213%27%2C%2762600%27%2C%2762601%27%2C%2762610%27%2C%2762611%27%2C%2762612%27%2C%2762613%27%2C%2762614%27%2C%2762615%27%2C%2762616%27%2C%2762617%27%2C%2762618%27%2C%2762619%27%2C%2762620%27%2C%2762621%27%2C%2762622%27%2C%2762623%27%2C%2762624%27%2C%2763158%27%2C%2763159%27%2C%2763160%27%2C%2763161%27%2C%2772019%27%2C%2772020%27%2C%2772150%27%2C%2772170%27%2C%2772171%27%2C%2772178%27%2C%2772199%27%2C%2772214%27%2C%2772215%27%2C%2772226%27%2C%2772227%27%2C%2772228%27%2C%2772229%27%2C%2772230%27%2C%2772231%27%2C%2772232%27%2C%2772251%27%2C%2772264%27%2C%2772265%27%2C%2772275%27%2C%2772276%27%2C%2772279%27%2C%2772292%27%2C%2772293%27%2C%2772335%27%2C%2772336%27%2C%2799019%27%2C%2799020%27%2C%2799065%27%2C%2799227%27%2C%2700062%27%2C%2700065%27%2C%2700072%27))&$select=AgencyCode,SiteNumber,SiteName,SiteTypeCode,Latitude,Longitude,CurrentConditionID,ParameterCode,TimeLocal,TimeZoneCode,Value,ValueFlagCode,RateOfChangeUnitPerHour,StatisticStatusCode,FloodStageStatusCode&$orderby=SiteNumber,AgencyCode,ParameterCode,TimeLocal%20desc&caller=National%20Water%20Dashboard%20default',\n",
    "        headers=headers_gwl,\n",
    "    )\n",
    "    \n",
    "    return response_gwl.json()['value']\n",
    "\n",
    "# dicts containing all the information that pertains to our surface water level data from USGS map\n",
    "# meaning that this is the network data that is called from https://dashboard.waterdata.usgs.gov/app/nwd/en/?region=lower48&aoi=default\n",
    "# This is all of the surface water level data that exists for USGS - the value that it refers to is gauge height level\n",
    "def getSurfaceWaterLevel():\n",
    "    headers_swl = {\n",
    "        'Accept': '*/*',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Referer': 'https://dashboard.waterdata.usgs.gov/app/nwd/en/?region=lower48&aoi=default',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36',\n",
    "        'sec-ch-ua': '\"Chromium\";v=\"112\", \"Google Chrome\";v=\"112\", \"Not:A-Brand\";v=\"99\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "    }\n",
    "\n",
    "    response_swl = requests.get(\n",
    "        'https://dashboard.waterdata.usgs.gov/service/cwis/1.0/odata/CurrentConditions?$top=15000&$filter=(AccessLevelCode%20eq%20%27P%27)%20and%20(1%20eq%201%20and%20true)%20and%20(SiteTypeCode%20in%20(%27ES%27%2C%27LK%27%2C%27OC%27%2C%27OC-CO%27%2C%27ST%27%2C%27ST-CA%27%2C%27ST-DCH%27%2C%27ST-TS%27%2C%27WE%27))%20and%20(ParameterCode%20in%20(%2700065%27))&$select=AgencyCode,SiteNumber,SiteName,SiteTypeCode,Latitude,Longitude,CurrentConditionID,ParameterCode,TimeLocal,TimeZoneCode,Value,ValueFlagCode,RateOfChangeUnitPerHour,StatisticStatusCode,FloodStageStatusCode&$orderby=SiteNumber,AgencyCode,ParameterCode,TimeLocal%20desc&caller=National%20Water%20Dashboard%20default',\n",
    "        headers=headers_swl,\n",
    "    )\n",
    "    \n",
    "    return response_swl.json()['value']\n",
    "\n",
    "# Here we are scraping all of the stream flow data from\n",
    "# https://dashboard.waterdata.usgs.gov/app/nwd/en/?region=lower48&aoi=default\n",
    "# the value we receive is discharge in cubic ft/s\n",
    "def getStreamFlow():\n",
    "    headers = {\n",
    "        'Accept': '*/*',\n",
    "        'Accept-Language': 'en-US,en;q=0.9',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Referer': 'https://dashboard.waterdata.usgs.gov/app/nwd/en/?region=lower48&aoi=default',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'same-origin',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36',\n",
    "        'sec-ch-ua': '\"Chromium\";v=\"112\", \"Google Chrome\";v=\"112\", \"Not:A-Brand\";v=\"99\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "    }\n",
    "\n",
    "    response_streamflow = requests.get(\n",
    "        'https://dashboard.waterdata.usgs.gov/service/cwis/1.0/odata/CurrentConditions?$top=15000&$filter=(AccessLevelCode%20eq%20%27P%27)%20and%20(1%20eq%201%20and%20true)%20and%20(SiteTypeCode%20in%20(%27ST%27%2C%27ST-CA%27%2C%27ST-DCH%27%2C%27ST-TS%27))%20and%20(ParameterCode%20in%20(%2730208%27%2C%2730209%27%2C%2750042%27%2C%2750050%27%2C%2750051%27%2C%2772137%27%2C%2772138%27%2C%2772139%27%2C%2772177%27%2C%2772243%27%2C%2774072%27%2C%2781395%27%2C%2799060%27%2C%2799061%27%2C%2700056%27%2C%2700058%27%2C%2700059%27%2C%2700060%27%2C%2700061%27))&$select=AgencyCode,SiteNumber,SiteName,SiteTypeCode,Latitude,Longitude,CurrentConditionID,ParameterCode,TimeLocal,TimeZoneCode,Value,ValueFlagCode,RateOfChangeUnitPerHour,StatisticStatusCode,FloodStageStatusCode&$orderby=SiteNumber,AgencyCode,ParameterCode,TimeLocal%20desc&caller=National%20Water%20Dashboard%20default',\n",
    "        headers=headers,\n",
    "    )\n",
    "    \n",
    "    return response_streamflow.json()['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d2d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all of our json responses from the webpages into dataframes\n",
    "#make some adjustments to column headers for clarity\n",
    "#more specifically - we want to specify what the values represent for each scraped \n",
    "#dataframe and what the rate of change represents\n",
    "\n",
    "swl_df = pd.DataFrame.from_dict(getSurfaceWaterLevel())\n",
    "streamflow_df = pd.DataFrame.from_dict(getStreamFlow())\n",
    "gwl_df = pd.DataFrame.from_dict(getGroundWaterLevel())\n",
    "spring_df = pd.DataFrame.from_dict(getSpringWaterLevel())\n",
    "precip_df = pd.DataFrame.from_dict(getPrecipitation())\n",
    "streamflow_df.rename(columns={'Value':'Discharge(ft^3/s)', 'RateOfChangeUnitPerHour':'RateofChange_Discharge(ft^3/s)'}, inplace=True)\n",
    "swl_df.rename(columns={'Value':'Gauge Height(ft)', 'RateOfChangeUnitPerHour':'RateofChange_Gauge(ft)'},inplace=True)\n",
    "gwl_df.rename(columns={'Value':'Depth to Water Below Land Surface(ft)', 'RateOfChangeUnitPerHour':'RateOfChange_groundwaterlevel(ft)'}, inplace=True)\n",
    "spring_df.rename(columns={'Value':'Gauge Height Spring(ft)', 'RateOfChangeUnitPerHour':'RateOfChange_springwaterlevel(ft)'}, inplace=True)\n",
    "precip_df.rename(columns={'Value':'Precipitation(in)', 'RateOfChangeUnitPerHour':'RateOfChangeUnitPerHour_precip(in)'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6905c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a method which combines all of the available data into one single source dataframe\n",
    "#This returns a dataframe that has all the characteristics available for each gauge\n",
    "#We can also combine the floodstage information so we know\n",
    "\n",
    "def DataMerge(streamflow,spring,surface,precipitation,ground):\n",
    "    merge_df = streamflow.combine_first(spring).combine_first(surface).combine_first(precipitation).combine_first(ground)\n",
    "    return merge_df\n",
    "\n",
    "#Make our merged data frame by calling DataMerge function - for all our non-flood status data at first\n",
    "merge = DataMerge(streamflow_df, spring_df, swl_df, precip_df, gwl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "720f2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if USGS Station name is only 7-digits long, then during parsing read_html cuts off the leading 0 and cast it as\n",
    "#an integer. We need to add that leading zero back on so our merge table and our flood stage table align\n",
    "\n",
    "df['USGSstationID'] = ['0' + str(value) if len(str(value)) == 7 else str(value) for value in df['USGSstationID']]\n",
    "df.rename(columns={'USGSstationID':'SiteNumber'}, inplace=True)\n",
    "\n",
    "#Now merge our dataframes on SiteNumber\n",
    "merge_final = pd.merge(merge,df,on='SiteNumber',how='outer')\n",
    "\n",
    "#We can drop all of the locations where merge_final does not have long/lat as we cannot use these\n",
    "merge_final = merge_final[(~merge_final['Latitude'].isna())&(~merge_final['Longitude'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61e01bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will create a SQLite3 database here so we can continually update the data we have with the most recent data\n",
    "#For instance, if we have a USGS gauge that is registering that it is in a flood stage - but there is no NWS\n",
    "#data that indicates what the flood stage for that particular part of the river is, then we will update it that value\n",
    "#with the current stage and streamflow (can also include some other parameters down the line if necessary)\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "#The data frame that we are going to use for this is the merge_final data frame which is a collection of \n",
    "#USGS gauge data as well as NWS data\n",
    "\n",
    "db_file = 'USGS_update.db'\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "#Create the dataframe that will house all of the historical data and current thresholds for \n",
    "#determining flood stage\n",
    "\n",
    "#Create a database that we can draw from that contains flood stage statuses\n",
    "hist_flood_ref_df = merge_final\n",
    "\n",
    "hist_flood_ref_df.to_sql('USGS_hist_data', conn, if_exists='replace', index=False)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0828010",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test to make sure our sqlite tbl is converting properly and appending\n",
    "conn = sqlite3.connect('USGS_update.db')\n",
    "z = pd.read_sql_query(\"SELECT * FROM USGS_hist_data\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cee71921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our flood risk adjustments include removing all USGS gauges that are in 'NOFLOOD', 'None', N/A\n",
    "#this narrows our scope to areas that are being reported as flooding\n",
    "floodRisk_df = merge_final[((merge_final['FloodStageStatusCode']!='NOFLOOD')|\n",
    "                            (merge_final['FloodStageStatusCode']!='None')|(~merge_final['FloodStageStatusCode'].isna()))\n",
    "                           |(~merge_final['NWSfloodstage(ft)'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6f0edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is National Water Model data and other geographic data found which will help fine tune our model\n",
    "fn_river = 'RouteLink_CONUS.nc' \n",
    "fn_river2 = 'newNWM.nc'\n",
    "fn_long_range = 'Fulldom_CONUS_LongRange.nc'\n",
    "\n",
    "#Opening NWM data using xarray - netCDF files should be opened using xarray\n",
    "data_river = xarray.open_dataset(fn_river)\n",
    "data_river2 = xarray.open_dataset(fn_river2)\n",
    "data_lr = xarray.open_dataset(fn_long_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03941efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the streamflow and node information coerced into a dataframe\n",
    "df_river = data_river.to_dataframe()\n",
    "df_river2 = data_river2.to_dataframe()\n",
    "\n",
    "#long range is data that has ~1km mesh to between elevation data, this will be our baseline for estimating the\n",
    "#flooding based on change from river elevation to nearest structure elevation\n",
    "long_range_df = data_lr.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5ab0894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to only retain all of the long_range data that is pertinent to us\n",
    "#We drop all the N/A where there is no TOPOGRAPHY or FLOWDIRECTION data\n",
    "#We also found through ARCGIS testing that if STREAMORDER == -9999.0 it is referencing non-river locations\n",
    "long_range_df = long_range_df[~long_range_df['FLOWDIRECTION'].isna()]\n",
    "long_range_df = long_range_df[~long_range_df['TOPOGRAPHY'].isna()]\n",
    "long_range_df = long_range_df[long_range_df['STREAMORDER'] != -9999.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad37ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We merge our NWM and other river information\n",
    "#This is the connection between our NWM data and geographic feature information\n",
    "merge_NWM = pd.merge(df_river, df_river2, left_on='link', right_on='feature_id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19ac69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import our elevation data into a usable dataset\n",
    "elevation_data = pd.read_csv('Louisiana_Stucture_Altitude.csv')\n",
    "\n",
    "#Remove the Nan values from our data\n",
    "elevation_data = elevation_data[~elevation_data['gridcode'].isna()]\n",
    "elevation_data = elevation_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e98216c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to find the nearest structures --> first to USGS flood stage warnings\n",
    "#This function is used to find the nearest structure to our USGS flood gauge\n",
    "from scipy.spatial import KDTree\n",
    "import geopy.distance\n",
    "\n",
    "def NearestStruct():\n",
    "    #Get our longitude and latitude values for our structures and our gauges\n",
    "    coords_flood = floodRisk_df[['Latitude','Longitude']].values\n",
    "    coords_struct = elevation_data[['latitude','longitude']].values\n",
    "    #Create a KDTree (used for efficiently finding the nearest neighbors to a point)\n",
    "    kdt = KDTree(coords_struct)\n",
    "    k_nearest = []\n",
    "    for i, row in floodRisk_df.iterrows():\n",
    "        gauge_point = row[['Latitude','Longitude']].values\n",
    "        _, indices = kdt.query(gauge_point, k=1)\n",
    "        nearest_struct = coords_struct[indices]\n",
    "        distance = geopy.distance.distance(gauge_point, nearest_struct).miles\n",
    "        struct = nearest_struct.flatten().tolist()\n",
    "        struct.append(distance)\n",
    "        struct.extend(list(gauge_point))\n",
    "        k_nearest.append(struct)\n",
    "    #Create our dataframe containing our coordinates and the distance between nearest building and gauge\n",
    "    #but only if the building is within 3 miles of the gauge\n",
    "    columns = ['Lat_nearest_struct', 'Long_nearest_struct', 'Distance(miles)_nearest_gauge', 'Lat_gauge', 'Long_gauge']\n",
    "    df_nearest = pd.DataFrame(k_nearest, columns=columns)\n",
    "    df_nearest = df_nearest[df_nearest['Distance(miles)_nearest_gauge'] < 3].reset_index()\n",
    "    \n",
    "    return df_nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caca1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now run our nearest structure function and assign the dataframe output\n",
    "#Since for now all of our elevation data is from Louisiana - all of the nearest structures will be within LA\n",
    "dfx = NearestStruct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a435e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our floodrisk_df contains all of our data for the gauges that currently read some level of flooding\n",
    "#our dfx dataframe contains the nearest location to all of the gauges in the US that read flooding,\n",
    "#however in our NearestStructure() definition we set the range of flood risk gauges to nearest structure\n",
    "#to 5000m so the furthest ourside of LA a gauge can possibly be is around 3miles\n",
    "#combined_df gives us the USGS information combined with the distance information we need\n",
    "combined_df = pd.merge(dfx, floodRisk_df, left_on='Lat_gauge', right_on='Latitude', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "941ea5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are only keeping the combined rows that have a Flood status warnings \n",
    "#Check why this doesn't retain the edits made by floodrisk_df?\n",
    "combined_df = combined_df[((combined_df['FloodStageStatusCode']!='NOFLOOD')&\n",
    "                            (combined_df['FloodStageStatusCode']!='None')&(~combined_df['FloodStageStatusCode'].isna())\n",
    "                  &(combined_df['FloodStageStatusCode']!='NA'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "210d30c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Lat_nearest_struct</th>\n",
       "      <th>Long_nearest_struct</th>\n",
       "      <th>Distance(miles)_nearest_gauge</th>\n",
       "      <th>Lat_gauge</th>\n",
       "      <th>Long_gauge</th>\n",
       "      <th>AgencyCode</th>\n",
       "      <th>CurrentConditionID</th>\n",
       "      <th>Depth to Water Below Land Surface(ft)</th>\n",
       "      <th>Discharge(ft^3/s)</th>\n",
       "      <th>...</th>\n",
       "      <th>TimeLocal</th>\n",
       "      <th>TimeZoneCode</th>\n",
       "      <th>ValueFlagCode</th>\n",
       "      <th>USGS station name</th>\n",
       "      <th>NWSfloodstage(ft)</th>\n",
       "      <th>Most recentstage(ft)</th>\n",
       "      <th>Most recentflow(cfs)</th>\n",
       "      <th>Historicalpeaks(cfs)</th>\n",
       "      <th>Most recentlocaldate/time</th>\n",
       "      <th>NWSstationID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5805</td>\n",
       "      <td>32.994423</td>\n",
       "      <td>-93.407545</td>\n",
       "      <td>0.638043</td>\n",
       "      <td>32.994581</td>\n",
       "      <td>-93.396560</td>\n",
       "      <td>USGS</td>\n",
       "      <td>62249.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-15T14:00:00Z</td>\n",
       "      <td>CDT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bayou Dorcheat near Springhill, LA</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.88</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>36700.0</td>\n",
       "      <td>2023-06-15 13:00:00</td>\n",
       "      <td>SPHL1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5885</td>\n",
       "      <td>29.693108</td>\n",
       "      <td>-91.208738</td>\n",
       "      <td>0.193392</td>\n",
       "      <td>29.692819</td>\n",
       "      <td>-91.211937</td>\n",
       "      <td>USGS</td>\n",
       "      <td>167820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59900.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-15T14:00:00Z</td>\n",
       "      <td>CDT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lower Atchafalaya River at Morgan City, LA</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.13</td>\n",
       "      <td>52400.0</td>\n",
       "      <td>512000.0</td>\n",
       "      <td>2023-06-15 13:00:00</td>\n",
       "      <td>MCGL1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5889</td>\n",
       "      <td>30.071343</td>\n",
       "      <td>-91.826405</td>\n",
       "      <td>0.173757</td>\n",
       "      <td>30.071035</td>\n",
       "      <td>-91.829284</td>\n",
       "      <td>USGS</td>\n",
       "      <td>278558.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>337.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-15T13:45:00Z</td>\n",
       "      <td>CDT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bayou Teche at Keystone L&amp;D nr St. Martinville...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.13</td>\n",
       "      <td>345.0</td>\n",
       "      <td>4430.0</td>\n",
       "      <td>2023-06-15 12:45:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  Lat_nearest_struct  Long_nearest_struct  \\\n",
       "10   5805           32.994423           -93.407545   \n",
       "59   5885           29.693108           -91.208738   \n",
       "63   5889           30.071343           -91.826405   \n",
       "\n",
       "    Distance(miles)_nearest_gauge  Lat_gauge  Long_gauge AgencyCode  \\\n",
       "10                       0.638043  32.994581  -93.396560       USGS   \n",
       "59                       0.193392  29.692819  -91.211937       USGS   \n",
       "63                       0.173757  30.071035  -91.829284       USGS   \n",
       "\n",
       "    CurrentConditionID  Depth to Water Below Land Surface(ft)  \\\n",
       "10             62249.0                                    NaN   \n",
       "59            167820.0                                    NaN   \n",
       "63            278558.0                                    NaN   \n",
       "\n",
       "    Discharge(ft^3/s)  ...             TimeLocal  TimeZoneCode  ValueFlagCode  \\\n",
       "10             1210.0  ...  2023-06-15T14:00:00Z           CDT            NaN   \n",
       "59            59900.0  ...  2023-06-15T14:00:00Z           CDT            NaN   \n",
       "63              337.0  ...  2023-06-15T13:45:00Z           CDT            NaN   \n",
       "\n",
       "                                    USGS station name  NWSfloodstage(ft)  \\\n",
       "10                 Bayou Dorcheat near Springhill, LA               11.0   \n",
       "59         Lower Atchafalaya River at Morgan City, LA                6.0   \n",
       "63  Bayou Teche at Keystone L&D nr St. Martinville...                NaN   \n",
       "\n",
       "   Most recentstage(ft)  Most recentflow(cfs)  Historicalpeaks(cfs)  \\\n",
       "10                11.88                1220.0               36700.0   \n",
       "59                 4.13               52400.0              512000.0   \n",
       "63                10.13                 345.0                4430.0   \n",
       "\n",
       "    Most recentlocaldate/time  NWSstationID  \n",
       "10        2023-06-15 13:00:00         SPHL1  \n",
       "59        2023-06-15 13:00:00         MCGL1  \n",
       "63        2023-06-15 12:45:00           NaN  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "784d41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each of our remaining USGS gauges (those which indicate some level of flooding) - we need \n",
    "#to create boudning boxes so that we can capture all of the NWM nodes that fall within that range\n",
    "def create_bounding_boxes(lat,lon,distance):\n",
    "    distance_meters = distance*1609.34\n",
    "    #latitude and longitude difference in meters\n",
    "    lat_diff = distance_meters/111111\n",
    "    lon_diff = distance_meters/(111111 * abs(math.cos(math.radians(lat))))\n",
    "    \n",
    "    #bounding boxes\n",
    "    lat_min = lat-lat_diff\n",
    "    lat_max = lat+lat_diff\n",
    "    lon_min = lon-lon_diff\n",
    "    lon_max = lon+lon_diff\n",
    "    \n",
    "    return [lat_min, lat_max, lon_min, lon_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "194d91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we actually generate all of the bouding box coordinates using our function above\n",
    "#This is for all of the USGS gauges of interest (flooding status indicated)\n",
    "#Note that this ONLY creates the bounding box coordinates and appends them as columns to combined_df\n",
    "\n",
    "#Create the columns to hold the bouding boxes\n",
    "combined_df['1mile']=None\n",
    "combined_df['3mile']=None\n",
    "combined_df['5mile']=None\n",
    "\n",
    "for index, row in combined_df.iterrows():\n",
    "    #Get the latitude and longitude we want to create our bounding boxes around (gauge lat/lon)\n",
    "    lat = row['Lat_gauge']\n",
    "    lon = row['Long_gauge']\n",
    "    \n",
    "    #Create the bouding boxes for each of the different 1,3,5 mile ranges we want\n",
    "    bbox_1mile = create_bounding_boxes(lat,lon,1)\n",
    "    bbox_3mile = create_bounding_boxes(lat,lon,3)\n",
    "    bbox_5mile = create_bounding_boxes(lat,lon,5)\n",
    "\n",
    "    #Append bouding boxes\n",
    "    combined_df.at[index,'1mile'] = bbox_1mile\n",
    "    combined_df.at[index,'3mile'] = bbox_3mile\n",
    "    combined_df.at[index,'5mile'] = bbox_5mile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "692daf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can create our get sub-dataframes from our NWM dataframe and append these onto our data frame\n",
    "#This is where we use the bounding boxes that we made and create actual dataframes that hold the relevant informatio\n",
    "#for instance we create dataframes that contain all of NWM points that fall within our bounding box\n",
    "#This can be converted into a function for greater abstraction\n",
    "\n",
    "#Create the columns which will house our in-radius NWM data\n",
    "combined_df['within_1mile_df'] = None\n",
    "combined_df['within_3mile_df'] = None\n",
    "combined_df['within_5mile_df'] = None\n",
    "\n",
    "#This is the in range long range data\n",
    "combined_df['within_1mile_lr_df'] = None\n",
    "combined_df['within_3mile_lr_df'] = None\n",
    "combined_df['within_5mile_lr_df'] = None\n",
    "\n",
    "\n",
    "for index, row in combined_df.iterrows():\n",
    "    \n",
    "    #Get the NWM nodes that fall within 1,3,5 mile radii that we have established around our USGS gauges\n",
    "    #First get our 1,3,5 mile radii for each of the corresponding rows in our in dataframe\n",
    "    lat_lon_1mile = create_bounding_boxes(row['Lat_gauge'], row['Long_gauge'], 1)\n",
    "    lat_lon_3mile = create_bounding_boxes(row['Lat_gauge'], row['Long_gauge'], 3)\n",
    "    lat_lon_5mile = create_bounding_boxes(row['Lat_gauge'], row['Long_gauge'], 5)\n",
    "    \n",
    "    ##################################################################################Turn into functions#########\n",
    "    #Create a df for each of our rows that contains only the NWM nodes within the radii\n",
    "    one_mile_df = merge_NWM[(merge_NWM['lat'] >= lat_lon_1mile[0])\n",
    "                            &(merge_NWM['lat'] <= lat_lon_1mile[1])\n",
    "                            &(merge_NWM['lon'] >= lat_lon_1mile[2])\n",
    "                            &(merge_NWM['lon'] <= lat_lon_1mile[3])]\n",
    "    three_mile_df = merge_NWM[(merge_NWM['lat'] >= lat_lon_3mile[0])\n",
    "                            &(merge_NWM['lat'] <= lat_lon_3mile[1])\n",
    "                            &(merge_NWM['lon'] >= lat_lon_3mile[2])\n",
    "                            &(merge_NWM['lon'] <= lat_lon_3mile[3])]\n",
    "    five_mile_df = merge_NWM[(merge_NWM['lat'] >= lat_lon_5mile[0])\n",
    "                            &(merge_NWM['lat'] <= lat_lon_5mile[1])\n",
    "                            &(merge_NWM['lon'] >= lat_lon_5mile[2])\n",
    "                            &(merge_NWM['lon'] <= lat_lon_5mile[3])]\n",
    "    \n",
    "    #These dataframes for the long_range data that we pulled\n",
    "    one_mile_lr_df = long_range_df[(long_range_df['LATITUDE'] >= lat_lon_1mile[0])\n",
    "                            &(long_range_df['LATITUDE'] <= lat_lon_1mile[1])\n",
    "                            &(long_range_df['LONGITUDE'] >= lat_lon_1mile[2])\n",
    "                            &(long_range_df['LONGITUDE'] <= lat_lon_1mile[3])]\n",
    "    three_mile_lr_df = long_range_df[(long_range_df['LATITUDE'] >= lat_lon_3mile[0])\n",
    "                            &(long_range_df['LATITUDE'] <= lat_lon_3mile[1])\n",
    "                            &(long_range_df['LONGITUDE'] >= lat_lon_3mile[2])\n",
    "                            &(long_range_df['LONGITUDE'] <= lat_lon_3mile[3])]\n",
    "    five_mile_lr_df = long_range_df[(long_range_df['LATITUDE'] >= lat_lon_5mile[0])\n",
    "                            &(long_range_df['LATITUDE'] <= lat_lon_5mile[1])\n",
    "                            &(long_range_df['LONGITUDE'] >= lat_lon_5mile[2])\n",
    "                            &(long_range_df['LONGITUDE'] <= lat_lon_5mile[3])]\n",
    "    \n",
    "    #Append our newly created dataframes onto our existing combined_df \n",
    "    combined_df.at[index,'within_1mile_df'] = one_mile_df\n",
    "    combined_df.at[index,'within_3mile_df'] = three_mile_df\n",
    "    combined_df.at[index,'within_5mile_df'] = five_mile_df\n",
    "    \n",
    "    #Append our newly created dataframes for the long range data onto our exisitng combined_df\n",
    "    combined_df.at[index,'within_1mile_lr_df'] = one_mile_lr_df\n",
    "    combined_df.at[index,'within_3mile_lr_df'] = three_mile_lr_df\n",
    "    combined_df.at[index,'within_5mile_lr_df'] = five_mile_lr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9b78908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We now have a combined_df that has a column containing a dataframe of all relevant NWM and long_range data\n",
    "#for each of the USGS gauges within a 1,3,5 miles radii\n",
    "\n",
    "#This is where we will need to make a loop for every single USGS gauge in with an active flood status stage code.\n",
    "#for now our test_df is using only the 5 mile radius NWM nodes and long range data for one of the USGS gauges\n",
    "test_df = combined_df['within_5mile_df'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d9491ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>alt</th>\n",
       "      <th>order</th>\n",
       "      <th>Qi</th>\n",
       "      <th>MusK</th>\n",
       "      <th>MusX</th>\n",
       "      <th>...</th>\n",
       "      <th>nCC</th>\n",
       "      <th>TopWdthCC</th>\n",
       "      <th>TopWdth</th>\n",
       "      <th>crs</th>\n",
       "      <th>streamflow</th>\n",
       "      <th>nudge</th>\n",
       "      <th>velocity</th>\n",
       "      <th>qSfcLatRunoff</th>\n",
       "      <th>qBucket</th>\n",
       "      <th>qBtmVertRunoff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1395955</th>\n",
       "      <td>19926733</td>\n",
       "      <td>0</td>\n",
       "      <td>19926747</td>\n",
       "      <td>-93.473274</td>\n",
       "      <td>33.050419</td>\n",
       "      <td>74.370003</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>18.338831</td>\n",
       "      <td>6.112943</td>\n",
       "      <td>b''</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.07684</td>\n",
       "      <td>0.06287</td>\n",
       "      <td>176.595008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396328</th>\n",
       "      <td>19934295</td>\n",
       "      <td>0</td>\n",
       "      <td>19934259</td>\n",
       "      <td>-93.327240</td>\n",
       "      <td>33.066174</td>\n",
       "      <td>85.260002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>9.347944</td>\n",
       "      <td>3.115981</td>\n",
       "      <td>b''</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396335</th>\n",
       "      <td>19934325</td>\n",
       "      <td>0</td>\n",
       "      <td>19934339</td>\n",
       "      <td>-93.389603</td>\n",
       "      <td>33.062504</td>\n",
       "      <td>71.459999</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>7.088737</td>\n",
       "      <td>2.362912</td>\n",
       "      <td>b''</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396337</th>\n",
       "      <td>19934349</td>\n",
       "      <td>0</td>\n",
       "      <td>19934355</td>\n",
       "      <td>-93.421318</td>\n",
       "      <td>33.056179</td>\n",
       "      <td>66.580002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>10.514965</td>\n",
       "      <td>3.504988</td>\n",
       "      <td>b''</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.02725</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396339</th>\n",
       "      <td>19934353</td>\n",
       "      <td>0</td>\n",
       "      <td>19934427</td>\n",
       "      <td>-93.410103</td>\n",
       "      <td>33.061363</td>\n",
       "      <td>69.989998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8.632882</td>\n",
       "      <td>2.877627</td>\n",
       "      <td>b''</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00032</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544441</th>\n",
       "      <td>19935279</td>\n",
       "      <td>0</td>\n",
       "      <td>19935747</td>\n",
       "      <td>-93.396454</td>\n",
       "      <td>32.999363</td>\n",
       "      <td>54.990002</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>84.409470</td>\n",
       "      <td>28.136490</td>\n",
       "      <td>b''</td>\n",
       "      <td>16.400000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544480</th>\n",
       "      <td>19935747</td>\n",
       "      <td>0</td>\n",
       "      <td>19935751</td>\n",
       "      <td>-93.395233</td>\n",
       "      <td>32.985126</td>\n",
       "      <td>54.349998</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>88.102333</td>\n",
       "      <td>29.367445</td>\n",
       "      <td>b''</td>\n",
       "      <td>35.599999</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00177</td>\n",
       "      <td>9.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544516</th>\n",
       "      <td>19935751</td>\n",
       "      <td>0</td>\n",
       "      <td>19935759</td>\n",
       "      <td>-93.388824</td>\n",
       "      <td>32.963844</td>\n",
       "      <td>53.849998</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>88.509064</td>\n",
       "      <td>29.503021</td>\n",
       "      <td>b''</td>\n",
       "      <td>31.169999</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00281</td>\n",
       "      <td>27.146001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544555</th>\n",
       "      <td>19935759</td>\n",
       "      <td>0</td>\n",
       "      <td>19935317</td>\n",
       "      <td>-93.374931</td>\n",
       "      <td>32.942425</td>\n",
       "      <td>53.599998</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>93.850655</td>\n",
       "      <td>31.283552</td>\n",
       "      <td>b''</td>\n",
       "      <td>38.859999</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>1.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544595</th>\n",
       "      <td>19935317</td>\n",
       "      <td>0</td>\n",
       "      <td>19935859</td>\n",
       "      <td>-93.370461</td>\n",
       "      <td>32.933697</td>\n",
       "      <td>53.599998</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>94.180634</td>\n",
       "      <td>31.393545</td>\n",
       "      <td>b''</td>\n",
       "      <td>39.289999</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             link  from        to        lon        lat        alt  order  \\\n",
       "1395955  19926733     0  19926747 -93.473274  33.050419  74.370003      1   \n",
       "1396328  19934295     0  19934259 -93.327240  33.066174  85.260002      1   \n",
       "1396335  19934325     0  19934339 -93.389603  33.062504  71.459999      1   \n",
       "1396337  19934349     0  19934355 -93.421318  33.056179  66.580002      1   \n",
       "1396339  19934353     0  19934427 -93.410103  33.061363  69.989998      1   \n",
       "...           ...   ...       ...        ...        ...        ...    ...   \n",
       "1544441  19935279     0  19935747 -93.396454  32.999363  54.990002      5   \n",
       "1544480  19935747     0  19935751 -93.395233  32.985126  54.349998      5   \n",
       "1544516  19935751     0  19935759 -93.388824  32.963844  53.849998      5   \n",
       "1544555  19935759     0  19935317 -93.374931  32.942425  53.599998      6   \n",
       "1544595  19935317     0  19935859 -93.370461  32.933697  53.599998      6   \n",
       "\n",
       "          Qi    MusK  MusX  ...   nCC  TopWdthCC    TopWdth  crs  streamflow  \\\n",
       "1395955  0.0  3600.0   0.2  ...  0.12  18.338831   6.112943  b''    0.770000   \n",
       "1396328  0.0  3600.0   0.2  ...  0.12   9.347944   3.115981  b''    0.000000   \n",
       "1396335  0.0  3600.0   0.2  ...  0.12   7.088737   2.362912  b''    0.000000   \n",
       "1396337  0.0  3600.0   0.2  ...  0.12  10.514965   3.504988  b''    0.030000   \n",
       "1396339  0.0  3600.0   0.2  ...  0.12   8.632882   2.877627  b''    0.000000   \n",
       "...      ...     ...   ...  ...   ...        ...        ...  ...         ...   \n",
       "1544441  0.0  3600.0   0.2  ...  0.10  84.409470  28.136490  b''   16.400000   \n",
       "1544480  0.0  3600.0   0.2  ...  0.10  88.102333  29.367445  b''   35.599999   \n",
       "1544516  0.0  3600.0   0.2  ...  0.10  88.509064  29.503021  b''   31.169999   \n",
       "1544555  0.0  3600.0   0.2  ...  0.10  93.850655  31.283552  b''   38.859999   \n",
       "1544595  0.0  3600.0   0.2  ...  0.10  94.180634  31.393545  b''   39.289999   \n",
       "\n",
       "         nudge velocity qSfcLatRunoff  qBucket  qBtmVertRunoff  \n",
       "1395955   0.00     0.32       0.07684  0.06287      176.595008  \n",
       "1396328   0.00     0.00       0.00000  0.00000        0.015000  \n",
       "1396335   0.00     0.00       0.00000  0.00000        0.001000  \n",
       "1396337   0.00     0.16       0.02725  0.00002        0.249000  \n",
       "1396339   0.00     0.07       0.00032  0.00000        0.007000  \n",
       "...        ...      ...           ...      ...             ...  \n",
       "1544441   0.00     0.93       0.00000  0.00002        0.000000  \n",
       "1544480   0.12     0.41       0.00000  0.00177        9.275000  \n",
       "1544516   0.00     0.15       0.00000  0.00281       27.146001  \n",
       "1544555   0.00     0.16       0.00000  0.00020        1.036000  \n",
       "1544595   0.00     0.16       0.00000  0.00000        0.000000  \n",
       "\n",
       "[116 rows x 30 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85365b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset the index for some upcoming calculations\n",
    "long_range_df.reset_index()\n",
    "\n",
    "#We only need the lat, lon and topo from these values so we can get these values alone in a new df\n",
    "a = long_range_df['LATITUDE'].values\n",
    "b = long_range_df['LONGITUDE'].values\n",
    "c = long_range_df['TOPOGRAPHY'].values\n",
    "longr_df = pd.DataFrame({'lat':a, 'lon':b, 'topo':c})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a40f7e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\3788006090.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['coordinates'] = test_df.apply(lambda row: (row['lat'], row['lon']), axis=1)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\3788006090.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['elevation'] = elevations\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\3788006090.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['distance'] = distances\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\3788006090.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['est_river_elevation'] = est_river_elevations\n"
     ]
    }
   ],
   "source": [
    "#This gives us the distance between each NWM node and its closest structure, the structure elevation,\n",
    "#and the estimated node elevation based on TOPO\n",
    "\n",
    "#Remember during testing we only have a subsample of the nodes within a 5 miles radius of our flooding USGS gauge\n",
    "\n",
    "from geopy.distance import geodesic\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "#reset the index\n",
    "longr_df.reset_index()\n",
    "\n",
    "# Convert latitude and longitude columns to tuples\n",
    "test_df['coordinates'] = test_df.apply(lambda row: (row['lat'], row['lon']), axis=1)\n",
    "elevation_data['coordinates'] = elevation_data.apply(lambda row: (row['latitude'], row['longitude']), axis=1)\n",
    "longr_df['coordinates'] = longr_df.apply(lambda row: (row['lat'], row['lon']), axis=1)\n",
    "\n",
    "\n",
    "# Create KDTrees for elevation and long range\n",
    "points = np.radians(elevation_data[['latitude', 'longitude']].values)\n",
    "kdtree = KDTree(points)\n",
    "\n",
    "#long range\n",
    "points2 = np.radians(longr_df[['lat','lon']].values)\n",
    "kdtree2 = KDTree(points2)\n",
    "\n",
    "#Find nearest latitude and longitude in elevation_data and long_range_data for each row in test_df\n",
    "elevations = []\n",
    "est_river_elevations = []\n",
    "distances = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    lat, lon = np.radians(row['lat']), np.radians(row['lon'])\n",
    "    _, idx = kdtree.query((lat, lon))\n",
    "    _, idx2 = kdtree2.query((lat, lon))\n",
    "    nearest_coords = elevation_data.loc[idx, 'coordinates']\n",
    "    nearest_coords2 = longr_df.loc[idx2, 'coordinates']\n",
    "    distance = geodesic(row['coordinates'], nearest_coords).meters\n",
    "    distance2 = geodesic(row['coordinates'], nearest_coords2).meters\n",
    "    distances.append(distance)\n",
    "    elevation = elevation_data.loc[idx, 'gridcode']\n",
    "    elevations.append(elevation)\n",
    "    est_river_elevation = longr_df.loc[idx2,'topo']\n",
    "    est_river_elevations.append(est_river_elevation)\n",
    "\n",
    "\n",
    "test_df['elevation'] = elevations\n",
    "test_df['distance'] = distances\n",
    "test_df['est_river_elevation'] = est_river_elevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc3dea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\41765664.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['latitude'] = test_df['coordinates'].apply(lat)\n",
      "C:\\Users\\ASKhazami\\AppData\\Local\\Temp\\ipykernel_13448\\41765664.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['longitude'] = test_df['coordinates'].apply(long)\n"
     ]
    }
   ],
   "source": [
    "#This is to get latitude and longitude as separate columns in our dataframe\n",
    "def lat(coord):\n",
    "    latitude,_ = coord\n",
    "    return latitude\n",
    "def long(coord):\n",
    "    _,longitude = coord\n",
    "    return longitude\n",
    "\n",
    "test_df['latitude'] = test_df['coordinates'].apply(lat)\n",
    "test_df['longitude'] = test_df['coordinates'].apply(long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bd711f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reseting the index here and dropping old index because our old index refers to feature_id which is out of order\n",
    "#with the dropped rows\n",
    "test_df = test_df.reset_index()\n",
    "test_df = test_df.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "daad9f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate distance between two coordinates using haversine formula\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Radius of the Earth in kilometers\n",
    "\n",
    "    # Convert latitude and longitude to radians\n",
    "    lat1_rad, lon1_rad, lat2_rad, lon2_rad = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Calculate differences between coordinates\n",
    "    d_lat = lat2_rad - lat1_rad\n",
    "    d_lon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Apply haversine formula\n",
    "    a = sin(d_lat / 2) ** 2 + cos(lat1_rad) * cos(lat2_rad) * sin(d_lon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "# Calculate the distance between consecutive coordinates and store it in a new column\n",
    "result_list = []\n",
    "for i in range(len(test_df)-1):\n",
    "    result = calculate_distance(test_df['latitude'][i], test_df['longitude'][i], test_df['latitude'][i+1],test_df['longitude'][i+1])\n",
    "    result_list.append(result*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bc302e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list.insert(0, 'N/A')\n",
    "#convert to meters\n",
    "test_df['distance_to_next_node'] = result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70e56428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>alt</th>\n",
       "      <th>order</th>\n",
       "      <th>Qi</th>\n",
       "      <th>MusK</th>\n",
       "      <th>MusX</th>\n",
       "      <th>...</th>\n",
       "      <th>qSfcLatRunoff</th>\n",
       "      <th>qBucket</th>\n",
       "      <th>qBtmVertRunoff</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>elevation</th>\n",
       "      <th>distance</th>\n",
       "      <th>est_river_elevation</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>distance_to_next_node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19926733</td>\n",
       "      <td>0</td>\n",
       "      <td>19926747</td>\n",
       "      <td>-93.473274</td>\n",
       "      <td>33.050419</td>\n",
       "      <td>74.370003</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07684</td>\n",
       "      <td>0.06287</td>\n",
       "      <td>176.595008</td>\n",
       "      <td>(33.050418853759766, -93.47327423095703)</td>\n",
       "      <td>74.0</td>\n",
       "      <td>5420.224484</td>\n",
       "      <td>76.0</td>\n",
       "      <td>33.050419</td>\n",
       "      <td>-93.473274</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19934295</td>\n",
       "      <td>0</td>\n",
       "      <td>19934259</td>\n",
       "      <td>-93.327240</td>\n",
       "      <td>33.066174</td>\n",
       "      <td>85.260002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>(33.0661735534668, -93.32723999023438)</td>\n",
       "      <td>82.0</td>\n",
       "      <td>7482.941518</td>\n",
       "      <td>80.0</td>\n",
       "      <td>33.066174</td>\n",
       "      <td>-93.327240</td>\n",
       "      <td>13721.836445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19934325</td>\n",
       "      <td>0</td>\n",
       "      <td>19934339</td>\n",
       "      <td>-93.389603</td>\n",
       "      <td>33.062504</td>\n",
       "      <td>71.459999</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>(33.062503814697266, -93.38960266113281)</td>\n",
       "      <td>75.0</td>\n",
       "      <td>7049.689356</td>\n",
       "      <td>74.0</td>\n",
       "      <td>33.062504</td>\n",
       "      <td>-93.389603</td>\n",
       "      <td>5825.751445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19934349</td>\n",
       "      <td>0</td>\n",
       "      <td>19934355</td>\n",
       "      <td>-93.421318</td>\n",
       "      <td>33.056179</td>\n",
       "      <td>66.580002</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02725</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.249000</td>\n",
       "      <td>(33.05617904663086, -93.42131805419922)</td>\n",
       "      <td>68.0</td>\n",
       "      <td>6112.030052</td>\n",
       "      <td>61.0</td>\n",
       "      <td>33.056179</td>\n",
       "      <td>-93.421318</td>\n",
       "      <td>3038.176122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19934353</td>\n",
       "      <td>0</td>\n",
       "      <td>19934427</td>\n",
       "      <td>-93.410103</td>\n",
       "      <td>33.061363</td>\n",
       "      <td>69.989998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00032</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>(33.061363220214844, -93.41010284423828)</td>\n",
       "      <td>62.0</td>\n",
       "      <td>6700.753981</td>\n",
       "      <td>63.0</td>\n",
       "      <td>33.061363</td>\n",
       "      <td>-93.410103</td>\n",
       "      <td>1193.614538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>19935279</td>\n",
       "      <td>0</td>\n",
       "      <td>19935747</td>\n",
       "      <td>-93.396454</td>\n",
       "      <td>32.999363</td>\n",
       "      <td>54.990002</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(32.99936294555664, -93.39645385742188)</td>\n",
       "      <td>75.0</td>\n",
       "      <td>980.457505</td>\n",
       "      <td>61.0</td>\n",
       "      <td>32.999363</td>\n",
       "      <td>-93.396454</td>\n",
       "      <td>145.945818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>19935747</td>\n",
       "      <td>0</td>\n",
       "      <td>19935751</td>\n",
       "      <td>-93.395233</td>\n",
       "      <td>32.985126</td>\n",
       "      <td>54.349998</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00177</td>\n",
       "      <td>9.275000</td>\n",
       "      <td>(32.98512649536133, -93.39523315429688)</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1015.712074</td>\n",
       "      <td>52.0</td>\n",
       "      <td>32.985126</td>\n",
       "      <td>-93.395233</td>\n",
       "      <td>1587.109611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>19935751</td>\n",
       "      <td>0</td>\n",
       "      <td>19935759</td>\n",
       "      <td>-93.388824</td>\n",
       "      <td>32.963844</td>\n",
       "      <td>53.849998</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00281</td>\n",
       "      <td>27.146001</td>\n",
       "      <td>(32.963844299316406, -93.38882446289062)</td>\n",
       "      <td>55.0</td>\n",
       "      <td>618.615517</td>\n",
       "      <td>52.0</td>\n",
       "      <td>32.963844</td>\n",
       "      <td>-93.388824</td>\n",
       "      <td>2440.815624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>19935759</td>\n",
       "      <td>0</td>\n",
       "      <td>19935317</td>\n",
       "      <td>-93.374931</td>\n",
       "      <td>32.942425</td>\n",
       "      <td>53.599998</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>1.036000</td>\n",
       "      <td>(32.94242477416992, -93.37493133544922)</td>\n",
       "      <td>66.0</td>\n",
       "      <td>865.605117</td>\n",
       "      <td>55.0</td>\n",
       "      <td>32.942425</td>\n",
       "      <td>-93.374931</td>\n",
       "      <td>2711.660308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>19935317</td>\n",
       "      <td>0</td>\n",
       "      <td>19935859</td>\n",
       "      <td>-93.370461</td>\n",
       "      <td>32.933697</td>\n",
       "      <td>53.599998</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>(32.93369674682617, -93.3704605102539)</td>\n",
       "      <td>66.0</td>\n",
       "      <td>334.426046</td>\n",
       "      <td>52.0</td>\n",
       "      <td>32.933697</td>\n",
       "      <td>-93.370461</td>\n",
       "      <td>1056.394602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         link  from        to        lon        lat        alt  order   Qi  \\\n",
       "0    19926733     0  19926747 -93.473274  33.050419  74.370003      1  0.0   \n",
       "1    19934295     0  19934259 -93.327240  33.066174  85.260002      1  0.0   \n",
       "2    19934325     0  19934339 -93.389603  33.062504  71.459999      1  0.0   \n",
       "3    19934349     0  19934355 -93.421318  33.056179  66.580002      1  0.0   \n",
       "4    19934353     0  19934427 -93.410103  33.061363  69.989998      1  0.0   \n",
       "..        ...   ...       ...        ...        ...        ...    ...  ...   \n",
       "111  19935279     0  19935747 -93.396454  32.999363  54.990002      5  0.0   \n",
       "112  19935747     0  19935751 -93.395233  32.985126  54.349998      5  0.0   \n",
       "113  19935751     0  19935759 -93.388824  32.963844  53.849998      5  0.0   \n",
       "114  19935759     0  19935317 -93.374931  32.942425  53.599998      6  0.0   \n",
       "115  19935317     0  19935859 -93.370461  32.933697  53.599998      6  0.0   \n",
       "\n",
       "       MusK  MusX  ...  qSfcLatRunoff  qBucket  qBtmVertRunoff  \\\n",
       "0    3600.0   0.2  ...        0.07684  0.06287      176.595008   \n",
       "1    3600.0   0.2  ...        0.00000  0.00000        0.015000   \n",
       "2    3600.0   0.2  ...        0.00000  0.00000        0.001000   \n",
       "3    3600.0   0.2  ...        0.02725  0.00002        0.249000   \n",
       "4    3600.0   0.2  ...        0.00032  0.00000        0.007000   \n",
       "..      ...   ...  ...            ...      ...             ...   \n",
       "111  3600.0   0.2  ...        0.00000  0.00002        0.000000   \n",
       "112  3600.0   0.2  ...        0.00000  0.00177        9.275000   \n",
       "113  3600.0   0.2  ...        0.00000  0.00281       27.146001   \n",
       "114  3600.0   0.2  ...        0.00000  0.00020        1.036000   \n",
       "115  3600.0   0.2  ...        0.00000  0.00000        0.000000   \n",
       "\n",
       "                                  coordinates  elevation     distance  \\\n",
       "0    (33.050418853759766, -93.47327423095703)       74.0  5420.224484   \n",
       "1      (33.0661735534668, -93.32723999023438)       82.0  7482.941518   \n",
       "2    (33.062503814697266, -93.38960266113281)       75.0  7049.689356   \n",
       "3     (33.05617904663086, -93.42131805419922)       68.0  6112.030052   \n",
       "4    (33.061363220214844, -93.41010284423828)       62.0  6700.753981   \n",
       "..                                        ...        ...          ...   \n",
       "111   (32.99936294555664, -93.39645385742188)       75.0   980.457505   \n",
       "112   (32.98512649536133, -93.39523315429688)       57.0  1015.712074   \n",
       "113  (32.963844299316406, -93.38882446289062)       55.0   618.615517   \n",
       "114   (32.94242477416992, -93.37493133544922)       66.0   865.605117   \n",
       "115    (32.93369674682617, -93.3704605102539)       66.0   334.426046   \n",
       "\n",
       "    est_river_elevation   latitude  longitude  distance_to_next_node  \n",
       "0                  76.0  33.050419 -93.473274                    N/A  \n",
       "1                  80.0  33.066174 -93.327240           13721.836445  \n",
       "2                  74.0  33.062504 -93.389603            5825.751445  \n",
       "3                  61.0  33.056179 -93.421318            3038.176122  \n",
       "4                  63.0  33.061363 -93.410103            1193.614538  \n",
       "..                  ...        ...        ...                    ...  \n",
       "111                61.0  32.999363 -93.396454             145.945818  \n",
       "112                52.0  32.985126 -93.395233            1587.109611  \n",
       "113                52.0  32.963844 -93.388824            2440.815624  \n",
       "114                55.0  32.942425 -93.374931            2711.660308  \n",
       "115                52.0  32.933697 -93.370461            1056.394602  \n",
       "\n",
       "[116 rows x 37 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97fb0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is where we make our calculations (Assuming topography is in est_river_elevation in m)\n",
    "#Distance is in meters as well\n",
    "#so our slope is change in elevation per meter -> which we then convert to degrees for \n",
    "#geometry calcs\n",
    "\n",
    "#Note this is also the simplified calculations that assume both river banks have the same slopes -\n",
    "#we will work on a more accurate formula down the line\n",
    "\n",
    "#Note: Channel Slope and Channel Side Slope are not the same, one refers to sides of channel other refers to the \n",
    "#slope of the whole channel eg. downwards slope of channel\n",
    "inland_flood = []\n",
    "flood_severity_ratio = []\n",
    "\n",
    "for _,row in test_df.iterrows():\n",
    "    #To get the area of each of our cross sections we draw some variables from our DF\n",
    "    top_width = row['TopWdth']\n",
    "    bottom_width = row['BtmWdth']\n",
    "    channel_slope = row['ChSlp']\n",
    "    slope = row['So']\n",
    "    cc_top_width = row['TopWdthCC']\n",
    "    \n",
    "    #Use the trapezoidal channel formula here to calculate our cross-sectional area of flow\n",
    "    #Reference for this math - Normal Depth, Manning's Equation, and Trapezoidal channels CE331 (hydraulic engineering)\n",
    "    flow_depth = ((top_width-bottom_width)/2)/channel_slope\n",
    "    area_cross_section = (bottom_width+channel_slope*flow_depth)*flow_depth\n",
    "    \n",
    "    #wetted perimeter is the distance along the inside of our trapezoidal channel that water is in contact with\n",
    "    #the channel sides and bottom\n",
    "    w = (1+(channel_slope)**2)**0.5\n",
    "    wetted_perimeter = bottom_width+(2*flow_depth*w)\n",
    "    \n",
    "    #Hydraulic radius is the area divided by the wetted perimeter\n",
    "    hyd_radius = area_cross_section/wetted_perimeter\n",
    "    \n",
    "    #hydraulic depth is the depth average (since it is a trapezoidal channel the depth is not uniform at all places)\n",
    "    #telling us what the depth would be if it were rectangular channel - We should use cross-sectional depth to \n",
    "    hyd_depth = area_cross_section/top_width\n",
    "    \n",
    "    #Find area using hyd_depth\n",
    "    area_hyd_depth = (bottom_width+channel_slope*hyd_depth)*hyd_depth\n",
    "    \n",
    "    #We will estimate the overflow area of the compound channel - making the assumption that slope channel slope is the same\n",
    "    #for both the inner and outer channel\n",
    "    flow_depth_cc = ((cc_top_width-top_width)/2)/channel_slope\n",
    "    est_cc_area = (bottom_width*channel_slope*flow_depth_cc)*flow_depth_cc\n",
    "    cc_hyd_depth = est_cc_area/cc_top_width\n",
    "    cc_hyd_area = (top_width*channel_slope*hyd_depth)*hyd_depth\n",
    "    est_tot_area = cc_hyd_area + area_hyd_depth\n",
    "    \n",
    "    #Average velocity of flow in our bottom channel cross section\n",
    "    #Using Mannings equation\n",
    "    avg_velocity = (1/row['n'])*hyd_radius**(2/3)*slope**(1/2)\n",
    "    \n",
    "    #Now we estimate the slope from the edge of body of water to the nearest structure to create a triangle\n",
    "    #from which we can calculate projected water level in meters\n",
    "    if(est_tot_area*1 < est_tot_area*avg_velocity*1):\n",
    "        \n",
    "        #find the overflow area cross-section\n",
    "        overflow = est_tot_area*avg_velocity*1 - est_tot_area*1 \n",
    "        #Get the slope from bank to nearest structure\n",
    "        #slope is done with a simple rise over run calculation\n",
    "        riv_build_slope = (row['elevation']-row['est_river_elevation'])/distance\n",
    "        #**************IMPORTANT****** the elevation on the sides of the river is NOT uniform this needs to be updated\n",
    "        #to reflect that \n",
    "        #get angle of slope in degrees - this currently assumes we have the same elevation on each side of the river \n",
    "        #need to update the code to reflect this\n",
    "        runoff_slope = math.atan(riv_build_slope)\n",
    "\n",
    "        #find the vertical distance above the compound channel water line our water advances\n",
    "        #We since the formula for area of our channel is a = (bottom_width + slope*height)*height we use \n",
    "        #quadratic formula to find height\n",
    "        a = cc_top_width\n",
    "        b = riv_build_slope\n",
    "        c = -(overflow)\n",
    "        d = (b**2)-(4*a*c)\n",
    "        m_above_waterline = (-b+math.sqrt(d))/(2*a)\n",
    "        \n",
    "        #Now that we have height we can use a simple angle calculation to find the distance up the waterline\n",
    "        #that the height corresponds too\n",
    "        distance_inland = abs(m_above_waterline/math.tan(runoff_slope))\n",
    "        inland_flood.append(distance_inland)\n",
    "        flood_severity_ratio.append(abs(distance_inland/row['distance']))     \n",
    "    else:\n",
    "        inland_flood.append('N/A')\n",
    "        flood_severity_ratio.append('N/A')\n",
    "    \n",
    "\n",
    "test_df['inland_flood'] = inland_flood\n",
    "test_df['severity_index'] = flood_severity_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f66cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The more advanced calculations should include the difference in slope between each side of the river and\n",
    "#an average change in the thickness of the river channel between nodes so we can determine bottlenecks\n",
    "\n",
    "#begin by finding the nearest eastern, western, northern and southern buildings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75cecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('flooding_numbers.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
